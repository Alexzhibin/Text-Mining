{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from KaggleWord2VecUtility import KaggleWord2VecUtility\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec.load(\"800features_50minwords_10context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for K Means clustering:  130.872732878 seconds.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import time\n",
    "\n",
    "start = time.time() # Start time\n",
    "\n",
    "# Set \"k\" (num_clusters) to be 1/5th of the vocabulary size, or an\n",
    "# average of 5 words per cluster\n",
    "word_vectors = model.syn0\n",
    "num_clusters = word_vectors.shape[0] / 5\n",
    "# Initalize a k-means object and use it to extract centroids\n",
    "kmeans_clustering = KMeans( n_clusters = num_clusters )\n",
    "idx = kmeans_clustering.fit_predict( word_vectors )\n",
    "\n",
    "# Get the end time and print how long the process took\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print \"Time taken for K Means clustering: \", elapsed, \"seconds.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a Word / Index dictionary, mapping each vocabulary word to\n",
    "# a cluster number                                                                                            \n",
    "word_centroid_map = dict(zip( model.index2word, idx ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 0\n",
      "[u'dave']\n",
      "\n",
      "Cluster 1\n",
      "[u'continuously', u'blank', u'shine', u'priority']\n",
      "\n",
      "Cluster 2\n",
      "[u'suit', u'perkins', u'employer', u'pao', u'kleiner', u'attend']\n",
      "\n",
      "Cluster 3\n",
      "[u'climb', u'considerable', u'upward', u'multiples', u'fundamentals', u'prospects']\n",
      "\n",
      "Cluster 4\n",
      "[u'therefore', u'fail', u'extent', u'argue', u'besides', u'ignore', u'crack', u'lie', u'lose', u'needle', u'otherwise', u'poorly', u'chances']\n",
      "\n",
      "Cluster 5\n",
      "[u'ambient', u'warm', u'intensity']\n",
      "\n",
      "Cluster 6\n",
      "[u'output', u'exports', u'imports']\n",
      "\n",
      "Cluster 7\n",
      "[u'nation', u'incentives', u'rules', u'residents', u'interests', u'entities', u'forcing', u'principles', u'tackle', u'privately', u'standards', u'groups', u'communities', u'workers', u'agreements', u'perks', u'branch']\n",
      "\n",
      "Cluster 8\n",
      "[u'axp', u'crm', u'sdrl', u'aig', u'mgm', u'gpro', u'wfm', u'gild']\n",
      "\n",
      "Cluster 9\n",
      "[u'concepts', u'logical', u'extensive', u'usability', u'simplicity']\n"
     ]
    }
   ],
   "source": [
    "# For the first 10 clusters\n",
    "for cluster in xrange(0,10):\n",
    "    #\n",
    "    # Print the cluster number  \n",
    "    print \"\\nCluster %d\" % cluster\n",
    "    #\n",
    "    # Find all of the words for that cluster number, and print them out\n",
    "    words = []\n",
    "    for i in xrange(0,len(word_centroid_map.values())):\n",
    "        if( word_centroid_map.values()[i] == cluster ):\n",
    "            words.append(word_centroid_map.keys()[i])\n",
    "    print words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_bag_of_centroids( wordlist, word_centroid_map ):\n",
    "    #\n",
    "    # The number of clusters is equal to the highest cluster index\n",
    "    # in the word / centroid map\n",
    "    num_centroids = max( word_centroid_map.values() ) + 1\n",
    "    #\n",
    "    # Pre-allocate the bag of centroids vector (for speed)\n",
    "    bag_of_centroids = np.zeros( num_centroids, dtype=\"float32\" )\n",
    "    #\n",
    "    # Loop over the words in the review. If the word is in the vocabulary,\n",
    "    # find which cluster it belongs to, and increment that cluster count \n",
    "    # by one\n",
    "    for word in wordlist:\n",
    "        if word in word_centroid_map:\n",
    "            index = word_centroid_map[word]\n",
    "            bag_of_centroids[index] += 1\n",
    "    #\n",
    "    # Return the \"bag of centroids\"\n",
    "    return bag_of_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read data from files \n",
    "article = pd.read_csv( \"train_trend_1.csv\")\n",
    "article_test = pd.read_csv( \"test_trend_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing train reviews...\n"
     ]
    }
   ],
   "source": [
    "print \"Parsing train reviews...\"\n",
    "\n",
    "opinions = []\n",
    "for opinion in article['Articles']:\n",
    "    opinions.append( \" \".join( KaggleWord2VecUtility.review_to_wordlist( opinion )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing test reviews...\n"
     ]
    }
   ],
   "source": [
    "print \"Parsing test reviews...\"\n",
    "\n",
    "opinions_test = []\n",
    "for opinion_test in article_test['Articles']:\n",
    "    opinions_test.append( \" \".join( KaggleWord2VecUtility.review_to_wordlist( opinion_test )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pre-allocate an array for the training set bags of centroids (for speed)\n",
    "train_centroids = np.zeros( (article['Articles'].size, num_clusters), \\\n",
    "    dtype=\"float32\" )\n",
    "\n",
    "# Transform the training set reviews into bags of centroids\n",
    "counter = 0\n",
    "for review in opinions:\n",
    "    train_centroids[counter] = create_bag_of_centroids( review, \\\n",
    "        word_centroid_map )\n",
    "    counter += 1\n",
    "\n",
    "# Repeat for test reviews \n",
    "test_centroids = np.zeros(( article_test['Articles'].size, num_clusters), \\\n",
    "    dtype=\"float32\" )\n",
    "\n",
    "counter = 0\n",
    "for review in opinions_test:\n",
    "    test_centroids[counter] = create_bag_of_centroids( review, \\\n",
    "        word_centroid_map )\n",
    "    counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting a random forest to labeled testing data...\n"
     ]
    }
   ],
   "source": [
    "# Fit a random forest and extract predictions\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "# Fitting the forest may take a few minutes\n",
    "print \"Fitting a random forest to labeled testing data...\"\n",
    "forest = forest.fit(train_centroids,article[\"trend\"])\n",
    "result_test = forest.predict(test_centroids)\n",
    "\n",
    "result_train = forest.predict(train_centroids)\n",
    "\n",
    "# Write the test results \n",
    "#output = pd.DataFrame(data={\"id\":test[\"id\"], \"sentiment\":result})\n",
    "#output.to_csv( \"BagOfCentroids.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testing_y = pd.read_csv(\"y_trend_1.csv\")\n",
    "training_y = pd.read_csv(\"train_trend_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[945 812]\n",
      " [907 803]]\n",
      "0.504182290164\n"
     ]
    }
   ],
   "source": [
    "cm_testing = confusion_matrix(testing_y,result_test)\n",
    "print(cm_testing)\n",
    "accuracy_testing = (cm_testing[0,0]+cm_testing[1,1])/float(sum(sum(cm_testing)))\n",
    "print accuracy_testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4067    6]\n",
      " [   5 4012]]\n",
      "0.998640296663\n"
     ]
    }
   ],
   "source": [
    "cm_training = confusion_matrix(training_y['trend'], result_train)\n",
    "print(cm_training)\n",
    "accuracy_training = (cm_training[0,0]+cm_training[1,1])/float(sum(sum(cm_training)))\n",
    "print accuracy_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
